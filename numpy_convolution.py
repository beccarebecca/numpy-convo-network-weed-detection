# -*- coding: utf-8 -*-
"""numpy convolution

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1svlAGIF9PNykPREJoWx5VzTDWI-ptLJH
"""

import numpy as np
import random
import cv2
# read soybean kaggle data set. balance data , and sort data to labels, and shuffle data.
#Iterate data one by one for training
size = 225
dataset = []
prelabels = ["soybeans", "broadleaf", "grass", "soil"]
labels = {"soybean": [1,0,0,0], "broadleaf": [0,1,0,0], "grass": [0,0,1,0],"soil": [0,0,0,1]}
base1 = #"/content/drive/My Drive/..."
base2 = #"/content/drive/My Drive/..."
base3 = #"/content/drive/My Drive/..."
base4 = #"/content/drive/My Drive/..."
# base is for each class until it is different by one intergar
for i in range(1103):
  
  #img1 = cv2.imread(" {} {} .jpeg". format(base1, i),0)
  #img1 = cv2.resize(img1, (size,size), interpolation = cv2.INTER_AREA)
  dataset.append([prelabels[0], img1])

  #img2 =cv2.imread(" {} {} .jpeg". format(base2, i),0)
  #img2 = cv2.resize(img2, (size,size), interpolation = cv2.INTER_AREA)
  dataset.append([prelabels[1], img2])

  #img3 = cv2.imread(" {} {} .jpeg". format(base3, i),0)
  #img3 = cv2.resize(img3, (size,size), interpolation = cv2.INTER_AREA)
  dataset.append([prelabels[2], img3])

  #img4 = cv2.imread(" {} {} .jpeg". format(base4, i),0)
  #img4 = cv2.resize(img4, (size,size), interpolation = cv2.INTER_AREA)
  dataset.append([prelabels[3], img4])
random.shuffle(dataset)
random.shuffle(dataset)

# finishing up the testing dataset, and traing dataset 
dataset = dataset[:-12]
testingData = dataset[-12:]

"""# Predict if a plant is a soybean crop, or just weeds from images with the help of my numpy convolutional network."""

paddingAdd =  np.pad(input, 4, pad_with, padder= 0)

numberColumns = input.shape[1]
firstNeurons = 150
hiddenNeurons = 75
outputNeurons = 4
bias = 1
@global relu1X 
@global relu2X
@global sigX
weights1 = np.random.randint(size = input, firstNeurons)
weight2 = np.random.randint(size = firstNeurons, hiddenNeurons)
weight3 = np.random.randint(size = hiddenNeurons, outputNeurons)
poolingNum = 3
convNum = 5

# a filter containing random numbe


def ranwind(num):
  num = np.random.randint(0,2, size = (num,num))
  return num



# leakey relu function
def Lrelu(x):
  if x >= 0:
    return 1
  return .03

#derivitive of leaky leaky relu
def derivRelu(x):
  if x <= 0:
    return .03
  return 1

#sigmoid function

def sigmoid(x):
  return 1/(1 + exp(-x))

# dervitive of sigmoid... output sigmoid times 1 minus output sugmoid
def sigDerivitive(o):
  return o * (1 -o)

# softmax function
def softmaxFunction(x):
  items = [np.exp(i) for i in x]
  sumEx = sum(items)
  

  softmax = [s/sumEx for s in items]
  return softmax

# log liklihood/ cross entropy loss function... the more wrong, the higher the number
def crossEntropy(x,t):
  # "t" = target , or image label, and "x" is predicted label
  t = i[1] for i in labels if i[0] == t
  for (x):
    j = sum( -{tlog(x) + (1 - t)log(l-x)})
    return j

# combination of the gradient derivitive of the the log-loss and softmax together
def softlogDrevitive():
  return (softmaxO - 1)

#convolutional layer taking the dot product between the ranwind
#matrix and thr input 5 by 5 at a time. Looping through the rows (slide) and columns

def convoLayer(input,filter):
  filterList = []
  filters = ranwind(convNum)if filter == None else filter
  for i in input:
    for j in numberColumns:
      filterList = filterList.append(filters.dot(np.transpose(input[i:i+6,j:j+6])))
  return filterList

def dropOut(n,input):
  input = input[!n]
  return input

# pooling layer computing the maximum of the output layer with the pooling window by 4 by 4 at a time

def poolMax(input):
   
  for i in input:
    for j in numberCoulumns:
      pooling = [max(input[i:i+poolingNum,j:j+poolingNum])]
      pooling.asarray
  
  return pooling

# fully connected network
def fullconnNet(element, input):
  bunch = []
  x = input
  x = input.dot(weight1) + bias
  hidden1 = Lrelu(x)
  x = hidden1.dot(weight2) + bias
  hidden2 = Lrelu(x)
  x = hidden2.dot(weight3) + bias
  x = softmaxfunction(x)
  pre = return i[0] for i in labels if i[1] == x
  return pre if len(element) == 1 else return(x,element[0])

#convolution layers
def convoNetworkLayers(x,filter1 = None, filter2 = None, filter3 = None):
  element = x
  x= element[1]
  x = 16 * (convoLayer(x,filter1))
  x = poolMax(x)
  relu1X = Lrelu(x) 
  x = 32 * (convoLayer(relu1X, filter2))
  x = poolMax(x)
  relu2X = Lrelu(x)
  x = 32 * (convoLayer(relu2X, filter3))
  x = dropOut(randrange(32),x)
  x = poolMax(x)
  x = np.flatten(x)
  sigX = sigmoid(x)
  fullconnNet(element, sigX)

# cost and dervitive calulations to begin to update the weights with gradient decient -parial dervivtives in respect to x
def updateweights(x,element):
  x = output
  error = crossEntropy(x,element)
  lr = .05
  weights3 =+ lr * error * softlogDrevitive(x) * np.(hidden2)
  hiddenError2 = error * softlogDrevitive(x) * weights3 * derivRelu(hidden2)
  weight2 =+ hiddenError2 * np.transpose(hidden1)
  hiddenError1 = error * softlogDrevitive(x) * weightsO * derivRelu(hidden2) * derivRelu(hidden1) * weightsh2
  weights3 =+ hiddenError1 * np.transpose(input)
  convOutputError = sigDerivitive(sigX) * hiddenError1
  filter3 = filter3 += convOutputError * np.transpose(relu2X)
  convHiddenError2 = convOutputError * filter3 * derivRelu(relu2X)
  filter2 = filter2 += convHiddenError2 * np.transpose(relu1X)
  convHiddenError1 = convHiddenError2 * filter2 * derivRelu(relu1X)
  filter1 = filter1 += convHiddenError1 * np.transpose(input)
  convoNetworkLayers(input, filter1, filter2, filter3)

# initiate training


def startTraining():
  for epoch in epochMax:
    for i in dataset:
      x, label = convoNetworkLayers(i) 
      updateweights(x, label)
  print("training is completed")

# testing 
def testing():
  count = 0
  for images in testingData:
    t = convoNetworkLayers(images[1])
    for i in labels:
      if i[1] == images[0]:
        l = i[0] 
        if l == t:
          count =+ 1
  percentage = (count/12)/100
  print("the training model is" ,percentage , " right ")



# add image and predict if it is a weed or soybean

def predict(x):
  ogImg = cv2.imread(x,1)
  cv2.imshow(ogimg)
  image = cv2.imread(x,0)
  image = cv2.resize(image, (size,size), interpolation = cv2.INTER_AREA)
  cv2.putText(ogImg, convoNetworkLayers(i))
  cv2.imshow(ogImg)